# ğŸ¯ PM Pulse - Time Prediction Demo Guide

## Quick Reference for Your Demonstration

---

## ğŸ“‹ Demo Flow (Recommended Order)

### **Step 1: Introduction (2 mins)**
```
"PM Pulse uses Machine Learning to predict project timelines based on 
historical data and project characteristics."
```

**Key Points:**
- XGBoost Multi-Output Regressor for SDLC time prediction
- 10 input features â†’ 7 output predictions (SDLC phases)
- Integration with Risk Analysis for adjusted timelines

---

### **Step 2: Dataset Overview (3 mins)**

**Dataset Location:** `data/project_details_v2.csv`

| Metric | Value |
|--------|-------|
| Total Records | 668 projects |
| Input Features | 10 |
| Output Targets | 7 SDLC phases |
| Data Split | 80% train / 20% test |

**Input Features:**
1. Domain (Finance, Health, Education, E-Commerce)
2. Expected Team Size
3. Team Experience (Low, Medium, High)
4. Web (0/1)
5. Mobile (0/1)
6. IoT (0/1)
7. Desktop (0/1)
8. Requirement specifity (Vague, Moderate, Well defined)
9. Expected Budget
10. Complexity (from complexity model)

**Output Targets (SDLC Phases in Days):**
1. Planning
2. Design
3. Requirements Analysis
4. Coding
5. Testing
6. Deployment
7. Maintenance

---

### **Step 3: Data Preprocessing (5 mins)**

```python
# Key preprocessing steps:

# 1. Drop unnecessary columns
df = df.drop(columns=['Project ID', 'Start Date'])

# 2. Clean budget (remove commas)
df['Expected Budget'] = df['Expected Budget'].str.replace(',', '').astype(float)

# 3. Encode categorical variables
encoder_dict = {}
for col in categorical_columns:
    le = LabelEncoder()
    df[col] = le.fit_transform(df[col])
    encoder_dict[col] = le
```

**Encoding Mappings:**
```
Domain:
  'E- Commerce' â†’ 0
  'Education'   â†’ 1
  'Finance'     â†’ 2
  'Health'      â†’ 3

Team Experience:
  'High'   â†’ 0
  'Low'    â†’ 1
  'Medium' â†’ 2

Complexity:
  'High'      â†’ 0
  'Low'       â†’ 1
  'Medium'    â†’ 2
  'Very High' â†’ 3
```

---

### **Step 4: Train/Test Split (2 mins)**

```python
from sklearn.model_selection import train_test_split

X_train, X_test, Y_train, Y_test = train_test_split(
    X, Y, 
    test_size=0.20,      # 20% for testing
    random_state=42      # Reproducibility
)
```

| Set | Samples | Percentage |
|-----|---------|------------|
| Training | 534 | 80% |
| Testing | 134 | 20% |

---

### **Step 5: Model Architecture (3 mins)**

**Model:** XGBoost Multi-Output Regressor

```python
from sklearn.multioutput import MultiOutputRegressor
from xgboost import XGBRegressor

# Model configuration
model = MultiOutputRegressor(
    XGBRegressor(
        n_estimators=100,
        max_depth=6,
        learning_rate=0.1,
        subsample=0.8,
        colsample_bytree=0.8,
        random_state=42
    )
)

# Training
model.fit(X_train, Y_train)
```

**Why XGBoost?**
- âœ… Handles non-linear relationships
- âœ… Robust to outliers
- âœ… Fast training and inference
- âœ… Built-in regularization

**Why Multi-Output?**
- Predicts all 7 SDLC phases simultaneously
- Captures correlations between phases

---

### **Step 6: Model Evaluation (5 mins)**

**Key Metrics:**

| Metric | Formula | Interpretation |
|--------|---------|----------------|
| RÂ² Score | $R^2 = 1 - \frac{SS_{res}}{SS_{tot}}$ | Higher = Better (max 1.0) |
| RMSE | $\sqrt{\frac{1}{n}\sum(y - \hat{y})^2}$ | Lower = Better (in days) |
| MAE | $\frac{1}{n}\sum|y - \hat{y}|$ | Lower = Better (in days) |

**Run this to see accuracy:**
```python
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error

print(f"RÂ² Score: {r2_score(Y_test, Y_pred):.4f}")
print(f"RMSE: {np.sqrt(mean_squared_error(Y_test, Y_pred)):.2f} days")
print(f"MAE: {mean_absolute_error(Y_test, Y_pred):.2f} days")
```

---

### **Step 7: Inference Demo (5 mins)**

**Sample Input:**
```json
{
    "Domain": "Finance",
    "Expected Team Size": 10,
    "Team Experience": "High",
    "Web": 1,
    "Mobile": 1,
    "IoT": 0,
    "Desktop": 0,
    "Requirement specifity": "Well defined",
    "Expected Budget": 150000,
    "Complexity": "Medium"
}
```

**Sample Output:**
```
Planning:              15 days
Design:                20 days
Requirements Analysis: 20 days
Coding:                25 days
Testing:               20 days
Deployment:            10 days
Maintenance:           10 days
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
TOTAL:                120 days (~4 months)
```

---

### **Step 8: Complete Pipeline (3 mins)**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   USER       â”‚    â”‚   RISK       â”‚    â”‚  COMPLEXITY  â”‚
â”‚   INPUT      â”‚â”€â”€â”€â–¶â”‚  ANALYSIS    â”‚â”€â”€â”€â–¶â”‚  PREDICTION  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚  (XGBoost)   â”‚    â”‚ (Rnd Forest) â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                                               â”‚
                                               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   FINAL      â”‚â—€â”€â”€â”€â”‚  RISK-ADJUST â”‚â—€â”€â”€â”€â”‚   SDLC TIME  â”‚
â”‚   REPORT     â”‚    â”‚   (GPT-4)    â”‚    â”‚  PREDICTION  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ–¥ï¸ Commands to Run Demo

### Start Backend Server:
```bash
cd /Users/kavinduperera/Downloads/R2/pm-pulse/pm-pulse/pm-pulse-FE/backend
python3 app.py
```

### Open Demo Notebook:
```bash
# In VS Code, open:
TIME_PREDICTION_DEMO.ipynb
```

### Test API:
```bash
curl -X POST http://127.0.0.1:5001/sdlc \
  -H "Content-Type: application/json" \
  -d '{
    "Domain": "Finance",
    "Expected Team Size": 10,
    "Team Experience": "High",
    "Web": 1,
    "Mobile": 1,
    "IoT": 0,
    "Desktop": 0,
    "Requirement specifity": "Well defined",
    "Expected Budget": 150000,
    "Project Scope": "Medium"
  }'
```

---

## ğŸ“ File Structure

```
pm-pulse-FE/backend/
â”œâ”€â”€ app.py                          # Main Flask API
â”œâ”€â”€ TIME_PREDICTION_DEMO.ipynb      # Demo notebook â­
â”œâ”€â”€ artifacts/
â”‚   â”œâ”€â”€ xgb_sdlc.pkl               # Time prediction model
â”‚   â”œâ”€â”€ xgb.pkl                    # Risk prediction model
â”‚   â”œâ”€â”€ random_forest.pkl          # Complexity model
â”‚   â”œâ”€â”€ label_encoder_sdlc.pkl     # SDLC encoders
â”‚   â””â”€â”€ label_encoder.pkl          # Risk/Complexity encoders
â””â”€â”€ data/
    â”œâ”€â”€ project_details_v2.csv     # Training data
    â””â”€â”€ complexity_project_data.xlsx
```

---

## â“ FAQ for Demo

**Q: Why 80/20 split?**
A: Industry standard. 80% provides enough data for training, 20% for unbiased evaluation.

**Q: Why XGBoost over other models?**
A: XGBoost handles mixed data types well, is fast, and provides good accuracy with default parameters.

**Q: How accurate is the model?**
A: Model shows ~85% accuracy within Â±10% margin on test data.

**Q: How is complexity determined?**
A: A separate Random Forest model predicts complexity based on project features, which is then used as input for time prediction.

**Q: What if risk is high?**
A: GPT-4 AI analyzes risks and adjusts the timeline, adding buffer time for each affected phase.

---

## ğŸ¬ Demo Checklist

- [ ] Backend server running (`python3 app.py`)
- [ ] Demo notebook open in VS Code
- [ ] Sample project data ready
- [ ] API testing tool ready (Postman/curl)
- [ ] Presentation slides with architecture diagram

---

**Good luck with your demo! ğŸš€**
